import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os

dir_path = os.getcwd()
dataset = pd.read_csv(dir_path+"\\Data.csv")
X = dataset.iloc[:, :-1].values #Include all the columns except the last one
y = dataset.iloc[:, -1].values #lAST ROW OF DATA

print(X)
print(y)

#df1 = pd.DataFrame(x[:,1:3])
nan1 =dataset.isnull().sum()
#df2 = pd.DataFrame(x[:,2])
#nan2 = df2.isnull().sum()

print(nan1)
#print(nan2)

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(X[:,1:3]) #Excludes the upper range (Columns 1 and 2).
X[:, 1:3] = imputer.transform(X[:,1:3])

print(X)

#Decode the text input
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')
X = np.array(ct.fit_transform(X)) #we need the format to be a numpy array for future modules input

print(X)

#Decode the depended variable as well since it is a text value
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y) #This is a vector that does not need to be transformed to a numpy array

print(y)

#Split the data into training and Verification sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) #random_state=1 ensure the same seed for random

print("\n Training Data:")
print(X_train)
print(y_train)
print("\n Test Data")
print(X_test)
print(y_test)

#Do not standardize the encoded columns.  Very little benefit, and it limits readibility.
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train[:,3:] = sc.fit_transform(X_train[:,3:])
#use the same scaling from the training set on the test set.  The training is what was used to develop the model.
X_test[:,3:]=sc.transform(X_test[:,3:])

print("Train\n", X_train)
print("Test\n", X_test)